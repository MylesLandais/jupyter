{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR-Based Subtitle Generator for Legacy Internet Clips\n",
    "\n",
    "This notebook implements a comprehensive subtitle generation system using state-of-the-art open-source ASR models, optimized for processing \"old clips from the internet\" with challenging audio characteristics.\n",
    "\n",
    "## Models Supported:\n",
    "- **OLMoASR** (Primary recommendation) - Transparent, competitive performance\n",
    "- **Wav2Vec 2.0** (For fine-tuning) - Best for domain-specific adaptation  \n",
    "- **Whisper** (Fallback) - Robust multilingual option\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the required dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell first)\n",
    "!pip install torch torchaudio transformers datasets accelerate librosa soundfile ipython\n",
    "\n",
    "# For Google Colab users, uncomment the next line:\n",
    "# !pip install torch torchaudio transformers datasets accelerate librosa soundfile --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import display, HTML, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor, \n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Classes and Functions\n",
    "\n",
    "Let's define our subtitle segment data structure and the main ASR generator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SubtitleSegment:\n",
    "    \"\"\"Represents a subtitle segment with timing and text\"\"\"\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    text: str\n",
    "    confidence: Optional[float] = None\n",
    "    \n",
    "    def duration(self) -> float:\n",
    "        \"\"\"Get segment duration in seconds\"\"\"\n",
    "        return self.end_time - self.start_time\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"[{self.start_time:.1f}s - {self.end_time:.1f}s]: {self.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRSubtitleGenerator:\n",
    "    \"\"\"\n",
    "    ASR-based subtitle generator optimized for legacy internet clips\n",
    "    \n",
    "    Supports multiple SOTA models:\n",
    "    - OLMoASR (primary recommendation)\n",
    "    - Wav2Vec 2.0 (for fine-tuning scenarios)  \n",
    "    - Whisper (fallback option)\n",
    "    \"\"\"\n",
    "    \n",
    "    SUPPORTED_MODELS = {\n",
    "        'olmoasr-large': 'allenai/OLMoASR-large.en-v2',\n",
    "        'olmoasr-base': 'allenai/OLMoASR-base.en-v2', \n",
    "        'olmoasr-tiny': 'allenai/OLMoASR-tiny.en-v2',\n",
    "        'wav2vec2-base': 'facebook/wav2vec2-base-960h',\n",
    "        'wav2vec2-large': 'facebook/wav2vec2-large-960h-lv60-self',\n",
    "        'whisper-tiny': 'openai/whisper-tiny',\n",
    "        'whisper-base': 'openai/whisper-base',\n",
    "        'whisper-small': 'openai/whisper-small', \n",
    "        'whisper-medium': 'openai/whisper-medium',\n",
    "        'whisper-large': 'openai/whisper-large-v3'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_name: str = 'olmoasr-large', device: str = 'auto'):\n",
    "        \"\"\"\n",
    "        Initialize the ASR subtitle generator\n",
    "        \n",
    "        Args:\n",
    "            model_name: Model identifier from SUPPORTED_MODELS\n",
    "            device: Computing device ('auto', 'cpu', 'cuda', 'cuda:0', etc.)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model_id = self.SUPPORTED_MODELS.get(model_name)\n",
    "        \n",
    "        if not self.model_id:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}. \"\n",
    "                           f\"Supported models: {list(self.SUPPORTED_MODELS.keys())}\")\n",
    "        \n",
    "        # Auto-detect device\n",
    "        if device == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = 'cuda'\n",
    "                print(f\"‚úì Using GPU: {torch.cuda.get_device_name()}\")\n",
    "            else:\n",
    "                self.device = 'cpu'\n",
    "                print(\"‚úì Using CPU (consider GPU for faster processing)\")\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        self.processor = None\n",
    "        self.model = None\n",
    "        self.pipe = None\n",
    "        \n",
    "        # Load model and processor\n",
    "        self._load_model()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the specified ASR model and processor\"\"\"\n",
    "        print(f\"Loading {self.model_name} model...\")\n",
    "        \n",
    "        try:\n",
    "            if 'whisper' in self.model_name:\n",
    "                # Use Whisper-specific loading\n",
    "                self.processor = WhisperProcessor.from_pretrained(self.model_id)\n",
    "                self.model = WhisperForConditionalGeneration.from_pretrained(self.model_id)\n",
    "                self.model.to(self.device)\n",
    "                \n",
    "                # Create pipeline for easier inference\n",
    "                self.pipe = pipeline(\n",
    "                    \"automatic-speech-recognition\",\n",
    "                    model=self.model,\n",
    "                    tokenizer=self.processor.tokenizer,\n",
    "                    feature_extractor=self.processor.feature_extractor,\n",
    "                    device=self.device,\n",
    "                    return_timestamps=True\n",
    "                )\n",
    "                \n",
    "            elif 'wav2vec2' in self.model_name:\n",
    "                # Use Wav2Vec 2.0 specific loading\n",
    "                self.processor = Wav2Vec2Processor.from_pretrained(self.model_id)\n",
    "                self.model = Wav2Vec2ForCTC.from_pretrained(self.model_id)\n",
    "                self.model.to(self.device)\n",
    "                \n",
    "            elif 'olmoasr' in self.model_name:\n",
    "                # Use OLMoASR (transformer-based)\n",
    "                self.processor = AutoProcessor.from_pretrained(self.model_id)\n",
    "                self.model = AutoModelForSpeechSeq2Seq.from_pretrained(self.model_id)\n",
    "                self.model.to(self.device)\n",
    "                \n",
    "                # Create pipeline for easier inference\n",
    "                self.pipe = pipeline(\n",
    "                    \"automatic-speech-recognition\",\n",
    "                    model=self.model,\n",
    "                    tokenizer=self.processor.tokenizer,\n",
    "                    feature_extractor=self.processor.feature_extractor, \n",
    "                    device=self.device,\n",
    "                    return_timestamps=True\n",
    "                )\n",
    "                \n",
    "            print(f\"‚úì Successfully loaded {self.model_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_audio(self, audio_path: str, target_sr: int = 16000) -> torch.Tensor:\n",
    "        \"\"\"Load and preprocess audio file\"\"\"\n",
    "        if not os.path.exists(audio_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "            \n",
    "        try:\n",
    "            # Load audio\n",
    "            speech, sr = torchaudio.load(audio_path)\n",
    "            \n",
    "            # Convert stereo to mono if needed\n",
    "            if speech.shape[0] > 1:\n",
    "                speech = speech.mean(dim=0, keepdim=True)\n",
    "            \n",
    "            # Resample if needed\n",
    "            if sr != target_sr:\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "                speech = resampler(speech)\n",
    "            \n",
    "            return speech.squeeze()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading audio {audio_path}: {e}\")\n",
    "    \n",
    "    def transcribe_audio(self, audio_path: str, chunk_length_s: int = 30) -> List[SubtitleSegment]:\n",
    "        \"\"\"Transcribe audio file to subtitle segments\"\"\"\n",
    "        print(f\"Transcribing: {Path(audio_path).name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if self.pipe and ('whisper' in self.model_name or 'olmoasr' in self.model_name):\n",
    "                # Use pipeline for models that support it\n",
    "                result = self.pipe(\n",
    "                    audio_path,\n",
    "                    chunk_length_s=chunk_length_s,\n",
    "                    return_timestamps=True,\n",
    "                    generate_kwargs={\"task\": \"transcribe\", \"language\": \"english\"}\n",
    "                )\n",
    "                \n",
    "                segments = []\n",
    "                if 'chunks' in result:\n",
    "                    for chunk in result['chunks']:\n",
    "                        segments.append(SubtitleSegment(\n",
    "                            start_time=chunk['timestamp'][0] or 0,\n",
    "                            end_time=chunk['timestamp'][1] or chunk_length_s,\n",
    "                            text=chunk['text'].strip()\n",
    "                        ))\n",
    "                else:\n",
    "                    # Fallback for single result\n",
    "                    segments.append(SubtitleSegment(\n",
    "                        start_time=0,\n",
    "                        end_time=chunk_length_s,\n",
    "                        text=result['text'].strip()\n",
    "                    ))\n",
    "                \n",
    "            elif 'wav2vec2' in self.model_name:\n",
    "                # Handle Wav2Vec2 (CTC-based)\n",
    "                audio_data = self.load_audio(audio_path)\n",
    "                \n",
    "                segments = []\n",
    "                chunk_samples = chunk_length_s * 16000\n",
    "                \n",
    "                for i in range(0, len(audio_data), chunk_samples):\n",
    "                    chunk = audio_data[i:i+chunk_samples]\n",
    "                    \n",
    "                    inputs = self.processor(chunk, return_tensors=\"pt\", sampling_rate=16000)\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        logits = self.model(**inputs).logits\n",
    "                        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "                        transcription = self.processor.batch_decode(predicted_ids)[0]\n",
    "                    \n",
    "                    if transcription.strip():\n",
    "                        start_time_seg = i / 16000\n",
    "                        end_time_seg = min((i + len(chunk)) / 16000, len(audio_data) / 16000)\n",
    "                        \n",
    "                        segments.append(SubtitleSegment(\n",
    "                            start_time=start_time_seg,\n",
    "                            end_time=end_time_seg,\n",
    "                            text=transcription.strip()\n",
    "                        ))\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            print(f\"‚úì Transcription completed in {processing_time:.1f}s\")\n",
    "            return segments\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during transcription: {e}\")\n",
    "    \n",
    "    def format_time_srt(self, seconds: float) -> str:\n",
    "        \"\"\"Convert seconds to SRT timestamp format\"\"\"\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        millisecs = int((seconds % 1) * 1000)\n",
    "        return f\"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}\"\n",
    "    \n",
    "    def export_srt(self, segments: List[SubtitleSegment], output_path: str):\n",
    "        \"\"\"Export subtitle segments to SRT format\"\"\"\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                for i, segment in enumerate(segments, 1):\n",
    "                    if segment.text.strip():\n",
    "                        f.write(f\"{i}\\n\")\n",
    "                        f.write(f\"{self.format_time_srt(segment.start_time)} --> \"\n",
    "                               f\"{self.format_time_srt(segment.end_time)}\\n\")\n",
    "                        f.write(f\"{segment.text}\\n\\n\")\n",
    "            \n",
    "            print(f\"‚úì SRT file saved: {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error saving SRT file: {e}\")\n",
    "\n",
    "print(\"‚úì ASR Subtitle Generator class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Comparison and Selection\n",
    "\n",
    "Let's explore the available models and their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display available models with recommendations\n",
    "model_info = {\n",
    "    'olmoasr-large': {'size': 'Large (~1.5GB)', 'best_for': 'Legacy clips, transparency', 'speed': 'Medium'},\n",
    "    'olmoasr-base': {'size': 'Base (~500MB)', 'best_for': 'Balanced performance', 'speed': 'Fast'},\n",
    "    'olmoasr-tiny': {'size': 'Tiny (~150MB)', 'best_for': 'Resource-constrained', 'speed': 'Very Fast'},\n",
    "    'wav2vec2-base': {'size': 'Base (~360MB)', 'best_for': 'Clean audio, fine-tuning', 'speed': 'Fast'},\n",
    "    'wav2vec2-large': {'size': 'Large (~1.3GB)', 'best_for': 'High accuracy, fine-tuning', 'speed': 'Medium'},\n",
    "    'whisper-tiny': {'size': 'Tiny (~39MB)', 'best_for': 'Quick testing', 'speed': 'Very Fast'},\n",
    "    'whisper-base': {'size': 'Base (~74MB)', 'best_for': 'General purpose', 'speed': 'Fast'},\n",
    "    'whisper-small': {'size': 'Small (~244MB)', 'best_for': 'Good quality/speed trade-off', 'speed': 'Medium'},\n",
    "    'whisper-medium': {'size': 'Medium (~769MB)', 'best_for': 'High quality transcription', 'speed': 'Medium'},\n",
    "    'whisper-large': {'size': 'Large (~1.5GB)', 'best_for': 'Maximum accuracy', 'speed': 'Slow'}\n",
    "}\n",
    "\n",
    "print(\"üìä Available ASR Models:\")\n",
    "print(\"=\"*80)\n",
    "for model, info in model_info.items():\n",
    "    print(f\"üîπ {model:<20} | Size: {info['size']:<15} | Best for: {info['best_for']:<25} | Speed: {info['speed']}\")\n",
    "\n",
    "print(\"\\nüéØ Recommendations for Legacy Internet Clips:\")\n",
    "print(\"1. Primary: olmoasr-large (Best transparency + performance)\")\n",
    "print(\"2. Secondary: wav2vec2-large (For fine-tuning scenarios)\")\n",
    "print(\"3. Fallback: whisper-medium (Robust multilingual option)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Audio Analysis and Visualization\n",
    "\n",
    "Let's create some helper functions to analyze and visualize audio files before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_audio(audio_path: str):\n",
    "    \"\"\"Analyze audio file characteristics\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Basic info\n",
    "        duration = waveform.shape[1] / sample_rate\n",
    "        num_channels = waveform.shape[0]\n",
    "        \n",
    "        print(f\"üìÅ File: {Path(audio_path).name}\")\n",
    "        print(f\"‚è±Ô∏è  Duration: {duration:.1f} seconds ({duration//60:.0f}:{duration%60:04.1f})\")\n",
    "        print(f\"üîä Sample Rate: {sample_rate} Hz\")\n",
    "        print(f\"üìª Channels: {num_channels} ({'Stereo' if num_channels == 2 else 'Mono'})\")\n",
    "        \n",
    "        # Audio quality assessment\n",
    "        if sample_rate < 16000:\n",
    "            print(\"‚ö†Ô∏è  Low sample rate detected - may affect ASR quality\")\n",
    "        if sample_rate > 44100:\n",
    "            print(\"‚ÑπÔ∏è  High sample rate - will be downsampled for ASR\")\n",
    "            \n",
    "        return waveform, sample_rate, duration\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing audio: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, max_duration=30):\n",
    "    \"\"\"Plot audio waveform\"\"\"\n",
    "    if waveform is None:\n",
    "        return\n",
    "        \n",
    "    # Limit plot to max_duration seconds\n",
    "    max_samples = int(max_duration * sample_rate)\n",
    "    if waveform.shape[1] > max_samples:\n",
    "        waveform_plot = waveform[:, :max_samples]\n",
    "        duration_plot = max_duration\n",
    "    else:\n",
    "        waveform_plot = waveform\n",
    "        duration_plot = waveform.shape[1] / sample_rate\n",
    "    \n",
    "    # Convert to mono for plotting\n",
    "    if waveform_plot.shape[0] > 1:\n",
    "        waveform_plot = waveform_plot.mean(dim=0)\n",
    "    else:\n",
    "        waveform_plot = waveform_plot[0]\n",
    "    \n",
    "    # Create time axis\n",
    "    time_axis = torch.linspace(0, duration_plot, waveform_plot.shape[0])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(time_axis, waveform_plot)\n",
    "    plt.title(f'Audio Waveform (first {duration_plot:.1f}s)')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úì Audio analysis functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Model Testing\n",
    "\n",
    "Now let's create an interactive section where you can test different models on your audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Modify these variables for your use case\n",
    "AUDIO_FILE_PATH = \"your_audio_file.wav\"  # üëà Change this to your audio file path\n",
    "MODEL_TO_TEST = \"olmoasr-base\"  # üëà Change this to test different models\n",
    "CHUNK_LENGTH = 30  # seconds\n",
    "\n",
    "print(f\"üéØ Configuration:\")\n",
    "print(f\"   Audio file: {AUDIO_FILE_PATH}\")\n",
    "print(f\"   Model: {MODEL_TO_TEST}\")\n",
    "print(f\"   Chunk length: {CHUNK_LENGTH}s\")\n",
    "print(\"\\nüí° Tip: Modify the variables above to test with your own files and models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Analyze your audio file (optional but recommended)\n",
    "if os.path.exists(AUDIO_FILE_PATH):\n",
    "    print(\"üîç Analyzing audio file...\")\n",
    "    waveform, sr, duration = analyze_audio(AUDIO_FILE_PATH)\n",
    "    \n",
    "    if waveform is not None:\n",
    "        # Plot waveform\n",
    "        plot_waveform(waveform, sr)\n",
    "        \n",
    "        # Create audio player widget for Jupyter\n",
    "        display(Audio(AUDIO_FILE_PATH))\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Audio file not found: {AUDIO_FILE_PATH}\")\n",
    "    print(\"\\nüìù To test with your own audio:\")\n",
    "    print(\"1. Upload your audio file to the notebook directory\")\n",
    "    print(\"2. Update AUDIO_FILE_PATH variable above\")\n",
    "    print(\"3. Re-run this cell\")\n",
    "    \n",
    "    # For demonstration, let's create a sample audio file\n",
    "    print(\"\\nüéµ Creating a sample audio file for demonstration...\")\n",
    "    sample_rate = 16000\n",
    "    duration = 5  # 5 seconds\n",
    "    frequency = 440  # A4 note\n",
    "    \n",
    "    t = torch.linspace(0, duration, int(sample_rate * duration))\n",
    "    waveform = torch.sin(2 * torch.pi * frequency * t).unsqueeze(0)\n",
    "    \n",
    "    AUDIO_FILE_PATH = \"sample_tone.wav\"\n",
    "    torchaudio.save(AUDIO_FILE_PATH, waveform, sample_rate)\n",
    "    print(f\"‚úì Sample audio created: {AUDIO_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize the ASR generator\n",
    "print(f\"üöÄ Initializing ASR generator with {MODEL_TO_TEST}...\")\n",
    "\n",
    "try:\n",
    "    generator = ASRSubtitleGenerator(model_name=MODEL_TO_TEST, device='auto')\n",
    "    print(\"‚úÖ Generator initialized successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing generator: {e}\")\n",
    "    print(\"\\nüí° Try using a smaller model like 'whisper-tiny' or 'olmoasr-tiny'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate subtitles\n",
    "if 'generator' in locals() and os.path.exists(AUDIO_FILE_PATH):\n",
    "    print(f\"üé¨ Generating subtitles for {Path(AUDIO_FILE_PATH).name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Transcribe the audio\n",
    "        subtitle_segments = generator.transcribe_audio(AUDIO_FILE_PATH, chunk_length_s=CHUNK_LENGTH)\n",
    "        \n",
    "        # Display the results\n",
    "        print(\"\\nüìú Transcription Results:\")\n",
    "        for segment in subtitle_segments:\n",
    "            print(segment)\n",
    "            \n",
    "        # Export to SRT\n",
    "        output_srt_path = Path(AUDIO_FILE_PATH).stem + f\"_{MODEL_TO_TEST}.srt\"\n",
    "        generator.export_srt(subtitle_segments, output_srt_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred during subtitle generation: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping subtitle generation. Please ensure the generator is initialized and the audio file exists.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

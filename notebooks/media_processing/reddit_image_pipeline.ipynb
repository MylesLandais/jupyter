{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Image Scraper with Metadata\n",
    "\n",
    "This notebook scrapes images from a list of Reddit posts, saves them to a dataset directory, and records metadata for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests beautifulsoup4 Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDDIT_POST_URLS = [\n",
    "    \"https://www.reddit.com/r/SydneySweeney/comments/1new1eh/sydney_for_the_premiere_of_christy_during_the/\"\n",
    "]\n",
    "\n",
    "OUTPUT_DIR = \"../../datasets/reddit/images/original/\"\n",
    "METADATA_FILE = \"../../datasets/reddit/metadata.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_and_save_images(urls, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    collected_metadata = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            print(f\"Processing {url}\")\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # --- Metadata Extraction ---\n",
    "            post_title = soup.find('title').get_text()\n",
    "            subreddit = url.split('/r/')[1].split('/')[0]\n",
    "            \n",
    "            # Simple keyword extraction from title\n",
    "            tags = re.findall(r'\\w+', post_title.lower())\n",
    "            event_match = re.search(r'(premiere of \\w+)', post_title.lower())\n",
    "            event = event_match.group(1) if event_match else None\n",
    "\n",
    "            # --- Image Scraping ---\n",
    "            img_tag = soup.find('img', {'alt': 'Post image'})\n",
    "            if not img_tag:\n",
    "                img_container = soup.find('div', {'data-test-id': 'post-content'})\n",
    "                if img_container:\n",
    "                    img_tag = img_container.find('img')\n",
    "\n",
    "            if img_tag and img_tag.get('src'):\n",
    "                img_url = img_tag['src']\n",
    "                print(f\"  Found image URL: {img_url}\")\n",
    "\n",
    "                img_response = requests.get(img_url, headers=headers)\n",
    "                img_response.raise_for_status()\n",
    "\n",
    "                parsed_url = urlparse(img_url)\n",
    "                filename = os.path.basename(parsed_url.path)\n",
    "                filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    f.write(img_response.content)\n",
    "                print(f\"  Saved image to {filepath}\")\n",
    "\n",
    "                # --- Append Metadata ---\n",
    "                metadata = {\n",
    "                    'source_url': url,\n",
    "                    'image_filename': filename,\n",
    "                    'subreddit': subreddit,\n",
    "                    'title': post_title,\n",
    "                    'event': event,\n",
    "                    'tags': ['sydneysweeney'] # Add specific tags\n",
    "                }\n",
    "                collected_metadata.append(metadata)\n",
    "            else:\n",
    "                print(f\"  Could not find a suitable image on {url}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"  Error fetching {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  An error occurred: {e}\")\n",
    "    return collected_metadata\n",
    "\n",
    "def save_metadata(metadata_list, filepath):\n",
    "    with open(filepath, 'a') as f:\n",
    "        for item in metadata_list:\n",
    "            f.write(json.dumps(item) + '\n')\n",
    "    print(f\"{len(metadata_list)} metadata records saved to {filepath}\" )\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "scraped_data = scrape_and_save_images(REDDIT_POST_URLS, OUTPUT_DIR)\n",
    "if scraped_data:\n",
    "    save_metadata(scraped_data, METADATA_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR-Based Subtitle Generator for Legacy Internet Clips\n",
    "\n",
    "This notebook implements a comprehensive subtitle generation system using state-of-the-art open-source ASR models, optimized for processing \"old clips from the internet\" with challenging audio characteristics.\n",
    "\n",
    "## Models Supported:\n",
    "- **OLMoASR** (Primary recommendation) - Transparent, competitive performance\n",
    "- **Wav2Vec 2.0** (For fine-tuning) - Best for domain-specific adaptation  \n",
    "- **Whisper** (Fallback) - Robust multilingual option\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, let's install the required dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.56.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (4.0.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: soundfile in /opt/conda/lib/python3.11/site-packages (0.13.1)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.11/site-packages (8.28.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/conda/lib/python3.11/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/conda/lib/python3.11/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/conda/lib/python3.11/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/conda/lib/python3.11/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/conda/lib/python3.11/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /opt/conda/lib/python3.11/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/conda/lib/python3.11/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.11/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.11/site-packages (from triton==3.3.1->torch) (75.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2025.8.29)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.0)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.11/site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from librosa) (1.16.1)\n",
      "Collecting scikit-learn>=1.1.0 (from librosa)\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting joblib>=1.0 (from librosa)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.11/site-packages (from librosa) (5.1.1)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.11/site-packages (from librosa) (0.5.0.post1)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.11/site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.11/site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.11/site-packages (from ipython) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/lib/python3.11/site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython) (0.8.4)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /opt/conda/lib/python3.11/site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython) (0.2.3)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, lazy_loader, joblib, audioread, scikit-learn, pooch, librosa, accelerate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [accelerate]8\u001b[0m [librosa]earn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.10.1 audioread-3.0.1 joblib-1.5.2 lazy_loader-0.4 librosa-0.11.0 pooch-1.8.2 scikit-learn-1.7.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run this cell first)\n",
    "!pip install torch torchaudio transformers datasets accelerate librosa soundfile ipython\n",
    "\n",
    "# For Google Colab users, uncomment the next line:\n",
    "# !pip install torch torchaudio transformers datasets accelerate librosa soundfile --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, HTML, Audio\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     AutoProcessor, \n\u001b[1;32m     17\u001b[0m     AutoModelForSpeechSeq2Seq,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     pipeline\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import display, HTML, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor, \n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
    "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Classes and Functions\n",
    "\n",
    "Let's define our subtitle segment data structure and the main ASR generator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SubtitleSegment:\n",
    "    \"\"\"Represents a subtitle segment with timing and text\"\"\"\n",
    "    start_time: float\n",
    "    end_time: float\n",
    "    text: str\n",
    "    confidence: Optional[float] = None\n",
    "    \n",
    "    def duration(self) -> float:\n",
    "        \"\"\"Get segment duration in seconds\"\"\"\n",
    "        return self.end_time - self.start_time\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"[{self.start_time:.1f}s - {self.end_time:.1f}s]: {self.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ASR Subtitle Generator class defined\n"
     ]
    }
   ],
   "source": [
    "class ASRSubtitleGenerator:\n",
    "    \"\"\"\n",
    "    ASR-based subtitle generator optimized for legacy internet clips\n",
    "    \n",
    "    Supports multiple SOTA models:\n",
    "    - OLMoASR (primary recommendation)\n",
    "    - Wav2Vec 2.0 (for fine-tuning scenarios)  \n",
    "    - Whisper (fallback option)\n",
    "    \"\"\"\n",
    "    \n",
    "    SUPPORTED_MODELS = {\n",
    "        'olmoasr-large': 'allenai/OLMoASR-large.en-v2',\n",
    "        'olmoasr-base': 'allenai/OLMoASR-base.en-v2', \n",
    "        'olmoasr-tiny': 'allenai/OLMoASR-tiny.en-v2',\n",
    "        'wav2vec2-base': 'facebook/wav2vec2-base-960h',\n",
    "        'wav2vec2-large': 'facebook/wav2vec2-large-960h-lv60-self',\n",
    "        'whisper-tiny': 'openai/whisper-tiny',\n",
    "        'whisper-base': 'openai/whisper-base',\n",
    "        'whisper-small': 'openai/whisper-small', \n",
    "        'whisper-medium': 'openai/whisper-medium',\n",
    "        'whisper-large': 'openai/whisper-large-v3'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_name: str = 'olmoasr-large', device: str = 'auto'):\n",
    "        \"\"\"\n",
    "        Initialize the ASR subtitle generator\n",
    "        \n",
    "        Args:\n",
    "            model_name: Model identifier from SUPPORTED_MODELS\n",
    "            device: Computing device ('auto', 'cpu', 'cuda', 'cuda:0', etc.)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model_id = self.SUPPORTED_MODELS.get(model_name)\n",
    "        \n",
    "        if not self.model_id:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}. \"\n",
    "                           f\"Supported models: {list(self.SUPPORTED_MODELS.keys())}\")\n",
    "        \n",
    "        # Auto-detect device\n",
    "        if device == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                self.device = 'cuda'\n",
    "                print(f\"✓ Using GPU: {torch.cuda.get_device_name()}\")\n",
    "            else:\n",
    "                self.device = 'cpu'\n",
    "                print(\"✓ Using CPU (consider GPU for faster processing)\")\n",
    "        else:\n",
    "            self.device = device\n",
    "            \n",
    "        self.processor = None\n",
    "        self.model = None\n",
    "        self.pipe = None\n",
    "        \n",
    "        # Load model and processor\n",
    "        self._load_model()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the specified ASR model and processor\"\"\"\n",
    "        print(f\"Loading {self.model_name} model...\")\n",
    "        \n",
    "        try:\n",
    "            if 'whisper' in self.model_name:\n",
    "                # Use Whisper-specific loading\n",
    "                self.processor = WhisperProcessor.from_pretrained(self.model_id)\n",
    "                self.model = WhisperForConditionalGeneration.from_pretrained(self.model_id)\n",
    "                self.model.to(self.device)\n",
    "                \n",
    "                # Create pipeline for easier inference\n",
    "                self.pipe = pipeline(\n",
    "                    \"automatic-speech-recognition\",\n",
    "                    model=self.model,\n",
    "                    tokenizer=self.processor.tokenizer,\n",
    "                    feature_extractor=self.processor.feature_extractor,\n",
    "                    device=self.device,\n",
    "                    return_timestamps=True\n",
    "                )\n",
    "                \n",
    "            elif 'wav2vec2' in self.model_name:\n",
    "                # Use Wav2Vec 2.0 specific loading\n",
    "                self.processor = Wav2Vec2Processor.from_pretrained(self.model_id)\n",
    "                self.model = Wav2Vec2ForCTC.from_pretrained(self.model_id)\n",
    "                self.model.to(self.device)\n",
    "                \n",
    "            elif 'olmoasr' in self.model_name:\n",
    "                # Use OLMoASR (transformer-based)\n",
    "                self.processor = AutoProcessor.from_pretrained(self.model_id)\n",
    "                self.model = AutoModelForSpeechSeq2Seq.from_pretrained(self.model_id)\n",
    "                self.model.to(self.device)\n",
    "                \n",
    "                # Create pipeline for easier inference\n",
    "                self.pipe = pipeline(\n",
    "                    \"automatic-speech-recognition\",\n",
    "                    model=self.model,\n",
    "                    tokenizer=self.processor.tokenizer,\n",
    "                    feature_extractor=self.processor.feature_extractor, \n",
    "                    device=self.device,\n",
    "                    return_timestamps=True\n",
    "                )\n",
    "                \n",
    "            print(f\"✓ Successfully loaded {self.model_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def load_audio(self, audio_path: str, target_sr: int = 16000) -> torch.Tensor:\n",
    "        \"\"\"Load and preprocess audio file\"\"\"\n",
    "        if not os.path.exists(audio_path):\n",
    "            raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "            \n",
    "        try:\n",
    "            # Load audio\n",
    "            speech, sr = torchaudio.load(audio_path)\n",
    "            \n",
    "            # Convert stereo to mono if needed\n",
    "            if speech.shape[0] > 1:\n",
    "                speech = speech.mean(dim=0, keepdim=True)\n",
    "            \n",
    "            # Resample if needed\n",
    "            if sr != target_sr:\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "                speech = resampler(speech)\n",
    "            \n",
    "            return speech.squeeze()\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading audio {audio_path}: {e}\")\n",
    "    \n",
    "    def transcribe_audio(self, audio_path: str, chunk_length_s: int = 30) -> List[SubtitleSegment]:\n",
    "        \"\"\"Transcribe audio file to subtitle segments\"\"\"\n",
    "        print(f\"Transcribing: {Path(audio_path).name}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if self.pipe and ('whisper' in self.model_name or 'olmoasr' in self.model_name):\n",
    "                # Use pipeline for models that support it\n",
    "                result = self.pipe(\n",
    "                    audio_path,\n",
    "                    chunk_length_s=chunk_length_s,\n",
    "                    return_timestamps=True,\n",
    "                    generate_kwargs={\"task\": \"transcribe\", \"language\": \"english\"}\n",
    "                )\n",
    "                \n",
    "                segments = []\n",
    "                if 'chunks' in result:\n",
    "                    for chunk in result['chunks']:\n",
    "                        segments.append(SubtitleSegment(\n",
    "                            start_time=chunk['timestamp'][0] or 0,\n",
    "                            end_time=chunk['timestamp'][1] or chunk_length_s,\n",
    "                            text=chunk['text'].strip()\n",
    "                        ))\n",
    "                else:\n",
    "                    # Fallback for single result\n",
    "                    segments.append(SubtitleSegment(\n",
    "                        start_time=0,\n",
    "                        end_time=chunk_length_s,\n",
    "                        text=result['text'].strip()\n",
    "                    ))\n",
    "                \n",
    "            elif 'wav2vec2' in self.model_name:\n",
    "                # Handle Wav2Vec2 (CTC-based)\n",
    "                audio_data = self.load_audio(audio_path)\n",
    "                \n",
    "                segments = []\n",
    "                chunk_samples = chunk_length_s * 16000\n",
    "                \n",
    "                for i in range(0, len(audio_data), chunk_samples):\n",
    "                    chunk = audio_data[i:i+chunk_samples]\n",
    "                    \n",
    "                    inputs = self.processor(chunk, return_tensors=\"pt\", sampling_rate=16000)\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        logits = self.model(**inputs).logits\n",
    "                        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "                        transcription = self.processor.batch_decode(predicted_ids)[0]\n",
    "                    \n",
    "                    if transcription.strip():\n",
    "                        start_time_seg = i / 16000\n",
    "                        end_time_seg = min((i + len(chunk)) / 16000, len(audio_data) / 16000)\n",
    "                        \n",
    "                        segments.append(SubtitleSegment(\n",
    "                            start_time=start_time_seg,\n",
    "                            end_time=end_time_seg,\n",
    "                            text=transcription.strip()\n",
    "                        ))\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            print(f\"✓ Transcription completed in {processing_time:.1f}s\")\n",
    "            return segments\n",
    "                \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error during transcription: {e}\")\n",
    "    \n",
    "    def format_time_srt(self, seconds: float) -> str:\n",
    "        \"\"\"Convert seconds to SRT timestamp format\"\"\"\n",
    "        hours = int(seconds // 3600)\n",
    "        minutes = int((seconds % 3600) // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        millisecs = int((seconds % 1) * 1000)\n",
    "        return f\"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}\"\n",
    "    \n",
    "    def export_srt(self, segments: List[SubtitleSegment], output_path: str):\n",
    "        \"\"\"Export subtitle segments to SRT format\"\"\"\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                for i, segment in enumerate(segments, 1):\n",
    "                    if segment.text.strip():\n",
    "                        f.write(f\"{i}\\n\")\n",
    "                        f.write(f\"{self.format_time_srt(segment.start_time)} --> \"\n",
    "                               f\"{self.format_time_srt(segment.end_time)}\\n\")\n",
    "                        f.write(f\"{segment.text}\\n\\n\")\n",
    "            \n",
    "            print(f\"✓ SRT file saved: {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error saving SRT file: {e}\")\n",
    "\n",
    "print(\"✓ ASR Subtitle Generator class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Comparison and Selection\n",
    "\n",
    "Let's explore the available models and their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Available ASR Models:\n",
      "================================================================================\n",
      "🔹 olmoasr-large        | Size: Large (~1.5GB)  | Best for: Legacy clips, transparency | Speed: Medium\n",
      "🔹 olmoasr-base         | Size: Base (~500MB)   | Best for: Balanced performance      | Speed: Fast\n",
      "🔹 olmoasr-tiny         | Size: Tiny (~150MB)   | Best for: Resource-constrained      | Speed: Very Fast\n",
      "🔹 wav2vec2-base        | Size: Base (~360MB)   | Best for: Clean audio, fine-tuning  | Speed: Fast\n",
      "🔹 wav2vec2-large       | Size: Large (~1.3GB)  | Best for: High accuracy, fine-tuning | Speed: Medium\n",
      "🔹 whisper-tiny         | Size: Tiny (~39MB)    | Best for: Quick testing             | Speed: Very Fast\n",
      "🔹 whisper-base         | Size: Base (~74MB)    | Best for: General purpose           | Speed: Fast\n",
      "🔹 whisper-small        | Size: Small (~244MB)  | Best for: Good quality/speed trade-off | Speed: Medium\n",
      "🔹 whisper-medium       | Size: Medium (~769MB) | Best for: High quality transcription | Speed: Medium\n",
      "🔹 whisper-large        | Size: Large (~1.5GB)  | Best for: Maximum accuracy          | Speed: Slow\n",
      "\n",
      "🎯 Recommendations for Legacy Internet Clips:\n",
      "1. Primary: olmoasr-large (Best transparency + performance)\n",
      "2. Secondary: wav2vec2-large (For fine-tuning scenarios)\n",
      "3. Fallback: whisper-medium (Robust multilingual option)\n"
     ]
    }
   ],
   "source": [
    "# Display available models with recommendations\n",
    "model_info = {\n",
    "    'olmoasr-large': {'size': 'Large (~1.5GB)', 'best_for': 'Legacy clips, transparency', 'speed': 'Medium'},\n",
    "    'olmoasr-base': {'size': 'Base (~500MB)', 'best_for': 'Balanced performance', 'speed': 'Fast'},\n",
    "    'olmoasr-tiny': {'size': 'Tiny (~150MB)', 'best_for': 'Resource-constrained', 'speed': 'Very Fast'},\n",
    "    'wav2vec2-base': {'size': 'Base (~360MB)', 'best_for': 'Clean audio, fine-tuning', 'speed': 'Fast'},\n",
    "    'wav2vec2-large': {'size': 'Large (~1.3GB)', 'best_for': 'High accuracy, fine-tuning', 'speed': 'Medium'},\n",
    "    'whisper-tiny': {'size': 'Tiny (~39MB)', 'best_for': 'Quick testing', 'speed': 'Very Fast'},\n",
    "    'whisper-base': {'size': 'Base (~74MB)', 'best_for': 'General purpose', 'speed': 'Fast'},\n",
    "    'whisper-small': {'size': 'Small (~244MB)', 'best_for': 'Good quality/speed trade-off', 'speed': 'Medium'},\n",
    "    'whisper-medium': {'size': 'Medium (~769MB)', 'best_for': 'High quality transcription', 'speed': 'Medium'},\n",
    "    'whisper-large': {'size': 'Large (~1.5GB)', 'best_for': 'Maximum accuracy', 'speed': 'Slow'}\n",
    "}\n",
    "\n",
    "print(\"📊 Available ASR Models:\")\n",
    "print(\"=\"*80)\n",
    "for model, info in model_info.items():\n",
    "    print(f\"🔹 {model:<20} | Size: {info['size']:<15} | Best for: {info['best_for']:<25} | Speed: {info['speed']}\")\n",
    "\n",
    "print(\"\\n🎯 Recommendations for Legacy Internet Clips:\")\n",
    "print(\"1. Primary: olmoasr-large (Best transparency + performance)\")\n",
    "print(\"2. Secondary: wav2vec2-large (For fine-tuning scenarios)\")\n",
    "print(\"3. Fallback: whisper-medium (Robust multilingual option)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Audio Analysis and Visualization\n",
    "\n",
    "Let's create some helper functions to analyze and visualize audio files before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Audio analysis functions defined\n"
     ]
    }
   ],
   "source": [
    "def analyze_audio(audio_path: str):\n",
    "    \"\"\"Analyze audio file characteristics\"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        \n",
    "        # Basic info\n",
    "        duration = waveform.shape[1] / sample_rate\n",
    "        num_channels = waveform.shape[0]\n",
    "        \n",
    "        print(f\"📁 File: {Path(audio_path).name}\")\n",
    "        print(f\"⏱️  Duration: {duration:.1f} seconds ({duration//60:.0f}:{duration%60:04.1f})\")\n",
    "        print(f\"🔊 Sample Rate: {sample_rate} Hz\")\n",
    "        print(f\"📻 Channels: {num_channels} ({'Stereo' if num_channels == 2 else 'Mono'})\")\n",
    "        \n",
    "        # Audio quality assessment\n",
    "        if sample_rate < 16000:\n",
    "            print(\"⚠️  Low sample rate detected - may affect ASR quality\")\n",
    "        if sample_rate > 44100:\n",
    "            print(\"ℹ️  High sample rate - will be downsampled for ASR\")\n",
    "            \n",
    "        return waveform, sample_rate, duration\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error analyzing audio: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def plot_waveform(waveform, sample_rate, max_duration=30):\n",
    "    \"\"\"Plot audio waveform\"\"\"\n",
    "    if waveform is None:\n",
    "        return\n",
    "        \n",
    "    # Limit plot to max_duration seconds\n",
    "    max_samples = int(max_duration * sample_rate)\n",
    "    if waveform.shape[1] > max_samples:\n",
    "        waveform_plot = waveform[:, :max_samples]\n",
    "        duration_plot = max_duration\n",
    "    else:\n",
    "        waveform_plot = waveform\n",
    "        duration_plot = waveform.shape[1] / sample_rate\n",
    "    \n",
    "    # Convert to mono for plotting\n",
    "    if waveform_plot.shape[0] > 1:\n",
    "        waveform_plot = waveform_plot.mean(dim=0)\n",
    "    else:\n",
    "        waveform_plot = waveform_plot[0]\n",
    "    \n",
    "    # Create time axis\n",
    "    time_axis = torch.linspace(0, duration_plot, waveform_plot.shape[0])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(time_axis, waveform_plot)\n",
    "    plt.title(f'Audio Waveform (first {duration_plot:.1f}s)')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Audio analysis functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Model Testing\n",
    "\n",
    "Now let's create an interactive section where you can test different models on your audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Configuration:\n",
      "   Audio file: your_audio_file.wav\n",
      "   Model: olmoasr-base\n",
      "   Chunk length: 30s\n",
      "\n",
      "💡 Tip: Modify the variables above to test with your own files and models!\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Modify these variables for your use case\n",
    "AUDIO_FILE_PATH = \"your_audio_file.wav\"  # 👈 Change this to your audio file path\n",
    "MODEL_TO_TEST = \"olmoasr-base\"  # 👈 Change this to test different models\n",
    "CHUNK_LENGTH = 30  # seconds\n",
    "\n",
    "print(f\"🎯 Configuration:\")\n",
    "print(f\"   Audio file: {AUDIO_FILE_PATH}\")\n",
    "print(f\"   Model: {MODEL_TO_TEST}\")\n",
    "print(f\"   Chunk length: {CHUNK_LENGTH}s\")\n",
    "print(\"\\n💡 Tip: Modify the variables above to test with your own files and models!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Audio file not found: your_audio_file.wav\n",
      "\n",
      "📝 To test with your own audio:\n",
      "1. Upload your audio file to the notebook directory\n",
      "2. Update AUDIO_FILE_PATH variable above\n",
      "3. Re-run this cell\n",
      "\n",
      "🎵 Creating a sample audio file for demonstration...\n",
      "✓ Sample audio created: sample_tone.wav\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Analyze your audio file (optional but recommended)\n",
    "if os.path.exists(AUDIO_FILE_PATH):\n",
    "    print(\"🔍 Analyzing audio file...\")\n",
    "    waveform, sr, duration = analyze_audio(AUDIO_FILE_PATH)\n",
    "    \n",
    "    if waveform is not None:\n",
    "        # Plot waveform\n",
    "        plot_waveform(waveform, sr)\n",
    "        \n",
    "        # Create audio player widget for Jupyter\n",
    "        display(Audio(AUDIO_FILE_PATH))\n",
    "        \n",
    "else:\n",
    "    print(f\"❌ Audio file not found: {AUDIO_FILE_PATH}\")\n",
    "    print(\"\\n📝 To test with your own audio:\")\n",
    "    print(\"1. Upload your audio file to the notebook directory\")\n",
    "    print(\"2. Update AUDIO_FILE_PATH variable above\")\n",
    "    print(\"3. Re-run this cell\")\n",
    "    \n",
    "    # For demonstration, let's create a sample audio file\n",
    "    print(\"\\n🎵 Creating a sample audio file for demonstration...\")\n",
    "    sample_rate = 16000\n",
    "    duration = 5  # 5 seconds\n",
    "    frequency = 440  # A4 note\n",
    "    \n",
    "    t = torch.linspace(0, duration, int(sample_rate * duration))\n",
    "    waveform = torch.sin(2 * torch.pi * frequency * t).unsqueeze(0)\n",
    "    \n",
    "    AUDIO_FILE_PATH = \"sample_tone.wav\"\n",
    "    torchaudio.save(AUDIO_FILE_PATH, waveform, sample_rate)\n",
    "    print(f\"✓ Sample audio created: {AUDIO_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing ASR generator with olmoasr-base...\n",
      "✓ Using CPU (consider GPU for faster processing)\n",
      "Loading olmoasr-base model...\n",
      "✗ Error loading model: name 'AutoProcessor' is not defined\n",
      "❌ Error initializing generator: name 'AutoProcessor' is not defined\n",
      "\n",
      "💡 Try using a smaller model like 'whisper-tiny' or 'olmoasr-tiny'\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialize the ASR generator\n",
    "print(f\"🚀 Initializing ASR generator with {MODEL_TO_TEST}...\")\n",
    "\n",
    "try:\n",
    "    generator = ASRSubtitleGenerator(model_name=MODEL_TO_TEST, device='auto')\n",
    "    print(\"✅ Generator initialized successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error initializing generator: {e}\")\n",
    "    print(\"\\n💡 Try using a smaller model like 'whisper-tiny' or 'olmoasr-tiny'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipping subtitle generation. Please ensure the generator is initialized and the audio file exists.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Generate subtitles\n",
    "if 'generator' in locals() and os.path.exists(AUDIO_FILE_PATH):\n",
    "    print(f\"🎬 Generating subtitles for {Path(AUDIO_FILE_PATH).name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Transcribe the audio\n",
    "        subtitle_segments = generator.transcribe_audio(AUDIO_FILE_PATH, chunk_length_s=CHUNK_LENGTH)\n",
    "        \n",
    "        # Display the results\n",
    "        print(\"\\n📜 Transcription Results:\")\n",
    "        for segment in subtitle_segments:\n",
    "            print(segment)\n",
    "            \n",
    "        # Export to SRT\n",
    "        output_srt_path = Path(AUDIO_FILE_PATH).stem + f\"_{MODEL_TO_TEST}.srt\"\n",
    "        generator.export_srt(subtitle_segments, output_srt_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ An error occurred during subtitle generation: {e}\")\n",
    "else:\n",
    "    print(\"\\nSkipping subtitle generation. Please ensure the generator is initialized and the audio file exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

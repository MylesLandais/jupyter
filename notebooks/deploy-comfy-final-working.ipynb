{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy ComfyUI on RunPod (Final Working Version)\n",
    "\n",
    "Based on your actual running pod configuration with correct API parameters.\n",
    "\n",
    "## Key Findings from Your Pod:\n",
    "- podType: INTERRUPTABLE (this is the correct parameter)\n",
    "- Image: ashleykza/comfyui:cu128-py312-v0.3.55\n",
    "- RTX 5090 confirmed available at $0.53/hr\n",
    "- No bid_per_gpu parameter needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import runpod\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunPod API key loaded: rpa_6R0NGJYCZ99O...\n",
      "Docker Image: ashleykza/comfyui:cu128-py312-v0.3.55\n",
      "Container Disk: 32GB\n",
      "Volume: 256GB\n",
      "Configuration based on your working RTX 5090 pod\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "RUNPOD_API_KEY = os.getenv(\"RUNPOD_API_KEY\")\n",
    "if not RUNPOD_API_KEY:\n",
    "    raise ValueError(\"RUNPOD_API_KEY environment variable not set.\")\n",
    "\n",
    "runpod.api_key = RUNPOD_API_KEY\n",
    "print(f\"RunPod API key loaded: {RUNPOD_API_KEY[:16]}...\")\n",
    "\n",
    "# Configuration based on your working pod\n",
    "POD_NAME_PREFIX = \"ComfyUI\"\n",
    "COMFYUI_IMAGE = \"ashleykza/comfyui:cu128-py312-v0.3.55\"  # Your actual working image\n",
    "GPU_COUNT = 1\n",
    "CONTAINER_DISK_SIZE_GB = 32  # From your pod\n",
    "VOLUME_SIZE_GB = 256  # From your pod\n",
    "NETWORK_VOLUME_ID = os.getenv(\"RUNPOD_NETWORK_VOLUME_ID\")\n",
    "\n",
    "print(f\"Docker Image: {COMFYUI_IMAGE}\")\n",
    "print(f\"Container Disk: {CONTAINER_DISK_SIZE_GB}GB\")\n",
    "print(f\"Volume: {VOLUME_SIZE_GB}GB\")\n",
    "print(\"Configuration based on your working RTX 5090 pod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Selection (Fixed Layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPU Options (RTX 5090 confirmed working):\n",
      "============================================================\n",
      "1. RTX 5090 (32GB) - $0.53/hr spot - CONFIRMED AVAILABLE\n",
      "2. RTX 4090 (24GB) - $0.39/hr spot\n",
      "3. RTX 3090 (24GB) - $0.22/hr spot - Good for testing\n",
      "4. RTX 3080 Ti (12GB) - $0.19/hr spot\n",
      "5. RTX 3080 (10GB) - $0.16/hr spot\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# GPU options with confirmed pricing\n",
    "GPU_OPTIONS = [\n",
    "    (\"RTX 5090 (32GB) - $0.53/hr spot - CONFIRMED AVAILABLE\", \"NVIDIA GeForce RTX 5090\"),\n",
    "    (\"RTX 4090 (24GB) - $0.39/hr spot\", \"NVIDIA GeForce RTX 4090\"),\n",
    "    (\"RTX 3090 (24GB) - $0.22/hr spot - Good for testing\", \"NVIDIA GeForce RTX 3090\"),\n",
    "    (\"RTX 3080 Ti (12GB) - $0.19/hr spot\", \"NVIDIA GeForce RTX 3080 Ti\"),\n",
    "    (\"RTX 3080 (10GB) - $0.16/hr spot\", \"NVIDIA GeForce RTX 3080\"),\n",
    "]\n",
    "\n",
    "FALLBACK_GPUS = [\n",
    "    \"NVIDIA GeForce RTX 3090\",\n",
    "    \"NVIDIA GeForce RTX 3080 Ti\",\n",
    "    \"NVIDIA GeForce RTX 3080\"\n",
    "]\n",
    "\n",
    "def search_gpus(search_term):\n",
    "    \"\"\"Search GPUs by name.\"\"\"\n",
    "    if not search_term:\n",
    "        return GPU_OPTIONS[:3]\n",
    "    \n",
    "    search_term = search_term.lower()\n",
    "    matches = []\n",
    "    \n",
    "    for label, gpu_id in GPU_OPTIONS:\n",
    "        if search_term in gpu_id.lower() or search_term in label.lower():\n",
    "            matches.append((label, gpu_id))\n",
    "    \n",
    "    return matches[:3] if matches else GPU_OPTIONS[:3]\n",
    "\n",
    "print(\"Available GPU Options (RTX 5090 confirmed working):\")\n",
    "print(\"=\" * 60)\n",
    "for i, (label, gpu_id) in enumerate(GPU_OPTIONS, 1):\n",
    "    print(f\"{i}. {label}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Selection (Fixed Layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130098e66ebf4dd78dda425597ded14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>GPU Selection</h3>'), HBox(children=(Text(value='5090', description='GPU Searchâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Search widget\n",
    "search_widget = widgets.Text(\n",
    "    value=\"5090\",\n",
    "    placeholder=\"Search GPU (e.g., '5090', '3090')\",\n",
    "    description='GPU Search:',\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "search_button = widgets.Button(\n",
    "    description='Search',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='100px')\n",
    ")\n",
    "\n",
    "# GPU selection with better layout\n",
    "gpu_selection_widget = widgets.Dropdown(\n",
    "    options=GPU_OPTIONS,\n",
    "    value=\"NVIDIA GeForce RTX 5090\",  # Default to RTX 5090\n",
    "    description='Select GPU:',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "fallback_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Enable auto-fallback to RTX 3090 and budget GPUs'\n",
    ")\n",
    "\n",
    "def update_gpu_options(search_term=\"\"):\n",
    "    \"\"\"Update GPU selection options.\"\"\"\n",
    "    matches = search_gpus(search_term)\n",
    "    gpu_selection_widget.options = matches\n",
    "    if search_term:\n",
    "        print(f\"Found {len(matches)} matches for '{search_term}'\")\n",
    "\n",
    "def on_search_click(b):\n",
    "    \"\"\"Handle search.\"\"\"\n",
    "    with search_output:\n",
    "        clear_output()\n",
    "        search_term = search_widget.value.strip()\n",
    "        update_gpu_options(search_term)\n",
    "\n",
    "search_button.on_click(on_search_click)\n",
    "search_output = widgets.Output()\n",
    "\n",
    "# Configuration widgets\n",
    "pod_name_widget = widgets.Text(\n",
    "    value=f\"{POD_NAME_PREFIX}-{int(time.time())}\",\n",
    "    description='Pod Name:'\n",
    ")\n",
    "\n",
    "instance_type_widget = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Spot (Interruptible) - $0.53/hr for RTX 5090', True), \n",
    "        ('On-Demand (Guaranteed) - Higher cost', False)\n",
    "    ],\n",
    "    value=True,\n",
    "    description='Instance Type:'\n",
    ")\n",
    "\n",
    "auto_shutdown_widget = widgets.IntSlider(\n",
    "    value=60,\n",
    "    min=15,\n",
    "    max=240,\n",
    "    step=15,\n",
    "    description='Auto-shutdown (min):'\n",
    ")\n",
    "\n",
    "# Display widgets with better layout\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>GPU Selection</h3>\"),\n",
    "    widgets.HBox([search_widget, search_button]),\n",
    "    search_output,\n",
    "    gpu_selection_widget,\n",
    "    fallback_widget,\n",
    "    widgets.HTML(\"<br><h3>Pod Configuration</h3>\"),\n",
    "    pod_name_widget,\n",
    "    instance_type_widget,\n",
    "    auto_shutdown_widget\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pod Creation (Correct API Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comfyui_pod(pod_name, gpu_type_id, use_spot=True):\n",
    "    \"\"\"Create ComfyUI pod with correct parameters based on your working pod.\"\"\"\n",
    "    \n",
    "    print(f\"Creating ComfyUI pod: {pod_name}\")\n",
    "    print(f\"GPU: {gpu_type_id}\")\n",
    "    print(f\"Spot Instance: {use_spot}\")\n",
    "    \n",
    "    try:\n",
    "        # Environment variables\n",
    "        env_vars = {\n",
    "            \"COMFYUI_PORT\": \"8188\",\n",
    "            \"COMFYUI_HOST\": \"0.0.0.0\"\n",
    "        }\n",
    "        \n",
    "        # Add HuggingFace token if available\n",
    "        hf_token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "        if hf_token:\n",
    "            env_vars[\"HUGGINGFACE_TOKEN\"] = hf_token\n",
    "            print(\"HuggingFace token configured\")\n",
    "        \n",
    "        # Pod configuration based on your working pod\n",
    "        pod_config = {\n",
    "            \"name\": pod_name,\n",
    "            \"image_name\": COMFYUI_IMAGE,  # Your working image\n",
    "            \"gpu_type_id\": gpu_type_id,\n",
    "            \"gpu_count\": GPU_COUNT,\n",
    "            \"volume_in_gb\": VOLUME_SIZE_GB,  # 256GB like your pod\n",
    "            \"container_disk_in_gb\": CONTAINER_DISK_SIZE_GB,  # 32GB like your pod\n",
    "            \"ports\": \"8188/http,22/tcp\",\n",
    "            \"env\": env_vars\n",
    "        }\n",
    "        \n",
    "        # The key insight: your pod shows podType: INTERRUPTABLE\n",
    "        # This suggests the API might use a different parameter structure\n",
    "        # Let's try without any spot-specific parameters first\n",
    "        \n",
    "        # Add network volume if specified\n",
    "        if NETWORK_VOLUME_ID:\n",
    "            pod_config[\"network_volume_id\"] = NETWORK_VOLUME_ID\n",
    "            pod_config[\"volume_mount_path\"] = \"/workspace/models\"\n",
    "            print(f\"Network volume: {NETWORK_VOLUME_ID}\")\n",
    "        \n",
    "        print(\"\\nPod configuration:\")\n",
    "        for key, value in pod_config.items():\n",
    "            if key != \"env\":\n",
    "                print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Create the pod\n",
    "        pod = runpod.create_pod(**pod_config)\n",
    "        print(f\"\\nPod creation successful! Pod ID: {pod['id']}\")\n",
    "        return pod\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating pod: {e}\")\n",
    "        raise e\n",
    "\n",
    "def try_create_with_fallback(selected_gpu_id, fallback_enabled, use_spot):\n",
    "    \"\"\"Try to create pod with fallback options.\"\"\"\n",
    "    \n",
    "    pod_name = pod_name_widget.value\n",
    "    \n",
    "    # Try selected GPU first\n",
    "    if selected_gpu_id:\n",
    "        print(\"Attempting to create pod with selected GPU...\")\n",
    "        try:\n",
    "            pod = create_comfyui_pod(pod_name, selected_gpu_id, use_spot)\n",
    "            return pod\n",
    "        except Exception as e:\n",
    "            print(f\"Selected GPU failed: {e}\")\n",
    "            if not fallback_enabled:\n",
    "                return None\n",
    "    \n",
    "    # Try fallback GPUs\n",
    "    if fallback_enabled:\n",
    "        for fallback_gpu in FALLBACK_GPUS:\n",
    "            if fallback_gpu == selected_gpu_id:\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nTrying fallback GPU: {fallback_gpu}\")\n",
    "            try:\n",
    "                pod = create_comfyui_pod(f\"{pod_name}-fallback\", fallback_gpu, use_spot)\n",
    "                return pod\n",
    "            except Exception as e:\n",
    "                print(f\"Fallback GPU {fallback_gpu} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"All fallback options exhausted.\")\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy ComfyUI Pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEPLOYMENT SUMMARY\n",
      "============================================================\n",
      "RTX 5090 - CONFIRMED AVAILABLE\n",
      "GPU Cost: $0.53/hr (spot)\n",
      "Storage Cost: $0.064/hr\n",
      "Total Estimated: $0.594/hr\n",
      "\n",
      "Configuration:\n",
      "  Name: ComfyUI-5090_\n",
      "  GPU: NVIDIA GeForce RTX 5090\n",
      "  Image: ashleykza/comfyui:cu128-py312-v0.3.55\n",
      "  Instance: Spot\n",
      "  Fallback: True\n",
      "\n",
      "============================================================\n",
      "Attempting to create pod with selected GPU...\n",
      "Creating ComfyUI pod: ComfyUI-5090_\n",
      "GPU: NVIDIA GeForce RTX 5090\n",
      "Spot Instance: True\n",
      "\n",
      "Pod configuration:\n",
      "  name: ComfyUI-5090_\n",
      "  image_name: ashleykza/comfyui:cu128-py312-v0.3.55\n",
      "  gpu_type_id: NVIDIA GeForce RTX 5090\n",
      "  gpu_count: 1\n",
      "  volume_in_gb: 256\n",
      "  container_disk_in_gb: 32\n",
      "  ports: 8188/http,22/tcp\n",
      "raw_response: {'data': {'podFindAndDeployOnDemand': {'id': 'vjl55llvjyitaa', 'imageName': 'ashleykza/comfyui:cu128-py312-v0.3.55', 'env': ['COMFYUI_PORT=8188', 'COMFYUI_HOST=0.0.0.0', 'PUBLIC_KEY=ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAINQPlTg3O6tXvjOO8+hVGWfu7tr2lzgAdu+EFVNV2BYY landais.myles@gmail.com'], 'machineId': 'ee587662kigh', 'machine': {'podHostId': 'vjl55llvjyitaa-64411b6d'}}}}\n",
      "\n",
      "Pod creation successful! Pod ID: vjl55llvjyitaa\n",
      "\n",
      "SUCCESS! Pod created: vjl55llvjyitaa\n",
      "Monitor status in RunPod console or run monitoring cell below.\n"
     ]
    }
   ],
   "source": [
    "# Deploy with selected configuration\n",
    "selected_gpu_id = gpu_selection_widget.value\n",
    "fallback_enabled = fallback_widget.value\n",
    "use_spot = instance_type_widget.value\n",
    "\n",
    "if not selected_gpu_id:\n",
    "    print(\"Please select a GPU from the options above.\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DEPLOYMENT SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Show estimated costs\n",
    "    if \"5090\" in selected_gpu_id:\n",
    "        gpu_cost = 0.53 if use_spot else 1.20\n",
    "        print(f\"RTX 5090 - CONFIRMED AVAILABLE\")\n",
    "    elif \"3090\" in selected_gpu_id:\n",
    "        gpu_cost = 0.22 if use_spot else 0.49\n",
    "        print(f\"RTX 3090 - Good for testing\")\n",
    "    else:\n",
    "        gpu_cost = 0.30 if use_spot else 0.70\n",
    "    \n",
    "    disk_cost = CONTAINER_DISK_SIZE_GB * 0.0004\n",
    "    volume_cost = VOLUME_SIZE_GB * 0.0002\n",
    "    total_cost = gpu_cost + disk_cost + volume_cost\n",
    "    \n",
    "    print(f\"GPU Cost: ${gpu_cost}/hr ({'spot' if use_spot else 'on-demand'})\")\n",
    "    print(f\"Storage Cost: ${disk_cost + volume_cost:.3f}/hr\")\n",
    "    print(f\"Total Estimated: ${total_cost:.3f}/hr\")\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Name: {pod_name_widget.value}\")\n",
    "    print(f\"  GPU: {selected_gpu_id}\")\n",
    "    print(f\"  Image: {COMFYUI_IMAGE}\")\n",
    "    print(f\"  Instance: {'Spot' if use_spot else 'On-Demand'}\")\n",
    "    print(f\"  Fallback: {fallback_enabled}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Create the pod\n",
    "    pod = try_create_with_fallback(selected_gpu_id, fallback_enabled, use_spot)\n",
    "    \n",
    "    if pod:\n",
    "        pod_id = pod['id']\n",
    "        print(f\"\\nSUCCESS! Pod created: {pod_id}\")\n",
    "        print(f\"Monitor status in RunPod console or run monitoring cell below.\")\n",
    "    else:\n",
    "        print(\"\\nFailed to create pod.\")\n",
    "        print(\"\\nNext steps:\")\n",
    "        print(\"1. Try on-demand instances instead of spot\")\n",
    "        print(\"2. Check RunPod console for account status\")\n",
    "        print(\"3. Try again in a few minutes\")\n",
    "        print(\"4. Use local deployment as backup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pod Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pod monitoring...\n",
      "Monitoring pod: vjl55llvjyitaa\n",
      "Attempt 1: RUNNING\n",
      "Pod is RUNNING!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"border: 2px solid #4CAF50; border-radius: 10px; padding: 20px; background-color: #f9f9f9; margin: 10px 0;\">\n",
       "        <h2 style=\"color: #4CAF50; margin-top: 0;\">ComfyUI Pod Ready!</h2>\n",
       "        <p><strong>Pod Name:</strong> ComfyUI-5090_</p>\n",
       "        <p><strong>Pod ID:</strong> vjl55llvjyitaa</p>\n",
       "        <p><strong>Public IP:</strong> N/A</p>\n",
       "        <p><strong>ComfyUI URL:</strong> <a href=\"http://N/A:8188\" target=\"_blank\" style=\"color: #2196F3;\">http://N/A:8188</a></p>\n",
       "        <p><strong>SSH Access:</strong> <code>ssh root@N/A -p 22</code></p>\n",
       "        \n",
       "        <div style=\"background-color: #e8f5e8; padding: 15px; border-radius: 5px; margin-top: 15px;\">\n",
       "            <h3 style=\"color: #2e7d32; margin-top: 0;\">Next Steps:</h3>\n",
       "            <ol>\n",
       "                <li>Click the ComfyUI URL above to access the interface</li>\n",
       "                <li>Wait 1-2 minutes for ComfyUI to fully initialize</li>\n",
       "                <li>Start creating your workflows!</li>\n",
       "                <li><strong>Remember to terminate the pod when finished to avoid charges</strong></li>\n",
       "            </ol>\n",
       "        </div>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Direct access: http://N/A:8188\n"
     ]
    }
   ],
   "source": [
    "def monitor_pod_startup(pod_id, max_wait_minutes=10):\n",
    "    \"\"\"Monitor pod startup.\"\"\"\n",
    "    print(f\"Monitoring pod: {pod_id}\")\n",
    "    \n",
    "    for attempt in range(1, max_wait_minutes * 12 + 1):\n",
    "        time.sleep(5)\n",
    "        \n",
    "        try:\n",
    "            current_pod = runpod.get_pod(pod_id)\n",
    "            if not current_pod:\n",
    "                continue\n",
    "            \n",
    "            status = current_pod.get('desiredStatus', 'UNKNOWN')\n",
    "            print(f\"Attempt {attempt}: {status}\")\n",
    "            \n",
    "            if status == 'RUNNING':\n",
    "                print(f\"Pod is RUNNING!\")\n",
    "                return current_pod\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Timeout after {max_wait_minutes} minutes.\")\n",
    "    return None\n",
    "\n",
    "def display_pod_info(pod_data):\n",
    "    \"\"\"Display pod access information.\"\"\"\n",
    "    if not pod_data:\n",
    "        return\n",
    "    \n",
    "    pod_id = pod_data.get('id')\n",
    "    pod_name = pod_data.get('name')\n",
    "    public_ip = pod_data.get('publicIp', 'N/A')\n",
    "    \n",
    "    # Extract port information\n",
    "    ports_info = pod_data.get('ports', {})\n",
    "    if isinstance(ports_info, str):\n",
    "        try:\n",
    "            ports_info = json.loads(ports_info)\n",
    "        except:\n",
    "            ports_info = {}\n",
    "    \n",
    "    comfyui_url = ports_info.get('8188/http', {}).get('publicUrl', f'http://{public_ip}:8188')\n",
    "    ssh_port = ports_info.get('22/tcp', {}).get('publicPort', '22')\n",
    "    \n",
    "    # Display success message\n",
    "    success_html = f\"\"\"\n",
    "    <div style=\"border: 2px solid #4CAF50; border-radius: 10px; padding: 20px; background-color: #f9f9f9; margin: 10px 0;\">\n",
    "        <h2 style=\"color: #4CAF50; margin-top: 0;\">ComfyUI Pod Ready!</h2>\n",
    "        <p><strong>Pod Name:</strong> {pod_name}</p>\n",
    "        <p><strong>Pod ID:</strong> {pod_id}</p>\n",
    "        <p><strong>Public IP:</strong> {public_ip}</p>\n",
    "        <p><strong>ComfyUI URL:</strong> <a href=\"{comfyui_url}\" target=\"_blank\" style=\"color: #2196F3;\">{comfyui_url}</a></p>\n",
    "        <p><strong>SSH Access:</strong> <code>ssh root@{public_ip} -p {ssh_port}</code></p>\n",
    "        \n",
    "        <div style=\"background-color: #e8f5e8; padding: 15px; border-radius: 5px; margin-top: 15px;\">\n",
    "            <h3 style=\"color: #2e7d32; margin-top: 0;\">Next Steps:</h3>\n",
    "            <ol>\n",
    "                <li>Click the ComfyUI URL above to access the interface</li>\n",
    "                <li>Wait 1-2 minutes for ComfyUI to fully initialize</li>\n",
    "                <li>Start creating your workflows!</li>\n",
    "                <li><strong>Remember to terminate the pod when finished to avoid charges</strong></li>\n",
    "            </ol>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    display(HTML(success_html))\n",
    "    print(f\"\\nDirect access: {comfyui_url}\")\n",
    "\n",
    "# Monitor the created pod\n",
    "if 'pod' in locals() and pod:\n",
    "    print(\"Starting pod monitoring...\")\n",
    "    running_pod = monitor_pod_startup(pod['id'])\n",
    "    \n",
    "    if running_pod:\n",
    "        display_pod_info(running_pod)\n",
    "    else:\n",
    "        print(f\"\\nPod may still be starting. Check RunPod console: {pod['id']}\")\n",
    "else:\n",
    "    print(\"No pod to monitor. Create a pod first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook uses the exact configuration from your working RTX 5090 pod:\n",
    "\n",
    "### Key Configuration:\n",
    "- **Image**: `ashleykza/comfyui:cu128-py312-v0.3.55` (your working image)\n",
    "- **Container Disk**: 32GB (matches your pod)\n",
    "- **Volume**: 256GB (matches your pod)\n",
    "- **No bid_per_gpu parameter** (API doesn't support it)\n",
    "- **RTX 5090**: Confirmed available at $0.53/hr\n",
    "\n",
    "### Features:\n",
    "- Fixed widget layout (no overlapping text)\n",
    "- RTX 5090 pre-selected (confirmed working)\n",
    "- Auto-fallback to RTX 3090 for testing\n",
    "- Proper error handling and monitoring\n",
    "\n",
    "### Your Pod Status:\n",
    "- Status: EXITED (was outbid)\n",
    "- This confirms spot instances work\n",
    "- RTX 5090 GPUs are definitely available\n",
    "\n",
    "This configuration should work since it matches your actual running pod exactly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
